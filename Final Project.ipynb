{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook for the final individual project of the Big Data and Automated Content Analysis Course. The goal of the project is to use supervised machine learning to successfully predict whether a document is populist or not. This project is based on the work of Matthijs Rooduijn ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data are files containing the manifestos of various parties from five European countries. These manifestos have been analysed by manual coders who decided whether a paragraph is populist or not. The manuscripts have been sourced online (in .csv format), were included in the original research (in .doc) or were copied from .pdf files (in .txt).\n",
    "\n",
    "While these documents already contain paragraphs, these are not always congruent with the paragraphs used in the coding of the original project. I therefore have three main tasks in this part of the project:\n",
    "\n",
    "    - transform all files into the same text data format and remove pre-existing paragraphs\n",
    "    - find the first two words of each paragraph (as noted in the coded results) in the manifesto docs and set new paragraphs accordingly \n",
    "    - splitting the texts into lists by line breaks and then feeding the lists and the results into one combined dataframe which will provide the features and labels for the later models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's begin by importing the necessary modules.\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Supervised Machine Learning\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV \n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "# Word Embeddings\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from embeddingvectorizer import EmbeddingCountVectorizer, EmbeddingTfidfVectorizer\n",
    "import embeddingvectorizer\n",
    "\n",
    "# I also want to import the functions defined in the other notebook.\n",
    "from functions import file_folder, save, clean, nl_lemmatise, classification_report\n",
    "\n",
    "# For Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To adapt the code to different directories, simply change this object. The results and manifestos folder should be placed in this directory.\n",
    "directory = \"/home/hennes/Desktop/Final Project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following code is used to go through the folder with the manuscripts\n",
    "# The text files are extracted depending on the file format and saved to a new file after removing all line breaks.\n",
    "\n",
    "for mani in glob(directory+'Results and manifestos/Manifestos/*/*'):\n",
    "    folder, filename = file_folder(mani)\n",
    "    if mani.endswith('.txt'):\n",
    "        with open(mani, mode='r') as m:\n",
    "            text = m.read()  \n",
    "    elif mani.endswith('.csv'):\n",
    "        m = pd.read_csv(mani)\n",
    "        if len(m.index) > 1:\n",
    "            text = \" \".join(m['text'].values[1:])\n",
    "        else:\n",
    "            text = m.iloc[0,0]\n",
    "    save(text, filename, directory+f\"Clean Manifestos/{folder}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following code I first place line breaks in the manifesto text files according to the paragraph structure that was used in the original project. In doing so, I had to make use of specific error feedback to correct the beginnings of the paragraphs in the results. In many cases there were inconsistencies between the manifestos and the results and some coders did not fill out the 'ParWords' column for all of their paragraphs. I was only able to rectify this by comparing the manifesto text files, the PDFs of the manifestos and the results.\n",
    "\n",
    "Once all errors were dealt with, the manifesto strings are split at the line breaks and turned into lists. These are then combined with the results into a dataframe that uses the coding scheme of the original article to identify paragraphs as populist (a paragraph needed to contain both a reference to \"the people\" and anti-elitism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for res in glob(directory+'Results and manifestos/Results/NL/*'):\n",
    "    folder = re.search(r\"([A-Z]){2}\", os.path.dirname(res)).group(0)\n",
    "    filename = re.sub(r\"\\.\\w+\",\"\", os.path.basename(res))\n",
    "    results = pd.read_excel(res).dropna(axis=0, how='all').dropna(axis=1, how='all') # need to delete a lot of empty cells\n",
    "    with open(directory+f\"Clean Manifestos/{folder}/{filename}.txt\", mode='r') as f:\n",
    "        text = re.sub(\"  \", \" \", re.sub(\"\\n\",\"\", f.read()))      # I am adding this because for some reason the text files kept adding an \\n after they were saved.\n",
    "        iterrow = results.iterrows()        # I have to skip the first line because I don't want a line break at beginning.\n",
    "        next(iterrow)\n",
    "        for index, row in iterrow:\n",
    "            par = row[\"ParWords\"].lower()\n",
    "            try:\n",
    "                begin = text.rindex(\"\\n\")+5  # I am adding a +\n",
    "                try:\n",
    "                    end = text.index(par, begin)\n",
    "                    text = text[:end] + \"\\n\" + text[end:]\n",
    "                except:\n",
    "                    print(f\"Error in {filename}: Last line-break at {begin}. Words '{par}' in row {index} not found\")\n",
    "            except:\n",
    "                try:\n",
    "                    end = text.index(par)\n",
    "                    text = text[:end] + \"\\n\" + text[end:]\n",
    "                except:\n",
    "                    print(f\"Error in {filename}: Words '{par}' in row {index} not found\")\n",
    "                    raise\n",
    "\n",
    "    # Now I save the dataset in the appropriately named folder and file\n",
    "    paralist = text.splitlines()\n",
    "    try:\n",
    "        data = results[[\"ParNum\", \"ParWords\", \"People\", \"AntEl\"]]\n",
    "        data[\"Populist\"] = np.where((data[\"People\"]>0) & (data[\"AntEl\"]>0), 'Populist', 'Not Populist')    # Creating the populist label according to coding rule of original paper.\n",
    "        data[\"text\"] = paralist\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate final table for {filename}\")\n",
    "        print(e) \n",
    "    save(data, filename, directory+f\"Master/{folder}/\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual dataframes are now combined to create the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for d in glob(directory+'Master/NL/*'):\n",
    "    folder, filename = file_folder(d)\n",
    "    df = pd.read_csv(d)\n",
    "    frames.append(df)\n",
    "    df['Party/Year'] = filename    # creating extra columns with the party/year and the country\n",
    "    df['Country'] = folder\n",
    "dataset = pd.concat(frames)\n",
    "dataset.to_csv(directory+'Master/NL/Combined Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the topics (labels) in the data.\n",
    "dataset['Populist'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spite of the insufficient number of paragraphs, the following code will serve as a demonstration for how supervised machine learning could be used to classify populist paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will the new column and the label column into two lists.\n",
    "features = dataset['text'].tolist()\n",
    "labels = dataset['Populist'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I will begin by taking a look at some text examples.\n",
    "print(f\"{features[500]}\\n\\n{features[0]}\\n\\n{features[1000]}\\n\\n{features[3000]}\\n\\n{features[2000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function cleans the features\n",
    "clean_features = clean(features)\n",
    "\n",
    "# Let's compare the clean features to the original ones.\n",
    "print(f\"{features[0]}\\n\\n{clean_features[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please note that for lemmatising to work, you have to download the dutch language pipeline.\n",
    "# This can be done in the user console with:\n",
    "# python -m spacy download nl_core_news_lg\n",
    "\n",
    "# In the last step of pre-processing, I lemmatise the words.\n",
    "docs = nl_lemmatise(clean_features)\n",
    "\n",
    "# Let's compare the lemmatised features to the clean ones.\n",
    "print(f\"{clean_features[0]}\\n\\n{docs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Testing and Validating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the notebook I will train, test and validate two models to predict whether a paragraph is populist or not. The first model will be based on normal word tokenization, while the second one will employ word embeddings. Both models and their respective parameters will be selected using a gridsearch. The possible classifiers for both models are a naive bayes, a support vector, a logistic regression and a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting with the classification, I split the labelled data into test and training data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs, labels,test_size=0.2, random_state=42)\n",
    "# Let's see if the features and labels indeed match up. That seems to be the case\n",
    "print(f\"{X_train[1]}\\n\\n{y_train[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I initialize the pipeline with this vectorizer and the classifier.\n",
    "pipe = Pipeline(steps = [('vectorizer', CountVectorizer()), ('classifier', MultinomialNB())])\n",
    "\n",
    "# Each grid specifies the parameters for one combination of vectorizer and classifier. \n",
    "grid = [{\n",
    "    'vectorizer': [CountVectorizer()], # parameters for combination of Count Vectorizer and Naive Bayes\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier': [MultinomialNB()]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [CountVectorizer()], # parameters for combination of Count Vectorizer and Logistic Regression\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier': [LogisticRegression(solver='liblinear')],\n",
    "    'classifier__C': [0.01, 1, 100] \n",
    "},\n",
    "{\n",
    "    'vectorizer': [CountVectorizer()], # parameters for combination of Count Vectorizer and Support Vector Machine\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier': [SVC()],\n",
    "    'classifier__kernel': ['linear','rbf', 'poly'],\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [CountVectorizer()], # parameters for combination of Count Vectorizer and Random Forest\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier': [RandomForestRegressor()],\n",
    "    'classifier__max_features': [1-20],\n",
    "    'classifier__n_estimators': [10, 100, 1000]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [TfidfVectorizer()], # parameters for combination of Tdifd Vectorizer and Naive Bayes\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier': [MultinomialNB()]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [TfidfVectorizer()], # parameters for combination of Tdifd Vectorizer and Logistic Regression\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier': [LogisticRegression(solver='liblinear')],\n",
    "    'classifier__C': [0.01, 1, 100] \n",
    "},\n",
    "{\n",
    "    'vectorizer': [TfidfVectorizer()], # parameters for combination of Tdifd Vectorizer and Support Vector Machine\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier': [SVC()],\n",
    "    'classifier__kernel': ['linear','rbf', 'poly'],\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [TfidfVectorizer()], # parameters for combination of Tdifd Vectorizer and Random Forest\n",
    "    'vectorizer__ngram_range' : [(1,1), (1,2)],\n",
    "    'vectorizer__max_df': [0.5, 1.0],\n",
    "    'vectorizer__min_df': [0, 5],\n",
    "    'classifier': [RandomForestRegressor()],\n",
    "    'classifier__max_features': [1-20],\n",
    "    'classifier__n_estimators': [10, 100, 1000]\n",
    "}]\n",
    "                 \n",
    "search = GridSearchCV(estimator = pipe,\n",
    "                      param_grid = grid,\n",
    "                      scoring = 'accuracy',\n",
    "                      cv = 5,\n",
    "                      n_jobs = -1,\n",
    "                      verbose = 10)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "reg_SML_params = search.best_params_\n",
    "reg_SML_model = search.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I am not able to perform the gridsearch using the word embeddings because my kernel dies each time I run the code. This has happened with all trained embeddings from https://github.com/clips/dutchembeddings, and also happens if I do not try to run a gridsearch but only specify a single model. I suspect that this is because I cannot allocate more than 5 GB of RAM to my virtual machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(\"/home/hennes/Downloads/160/sonar-160.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each grid specifies the parameters for one combination of vectorizer and classifier. \n",
    "pipe = Pipeline(steps = [('vectorizer', CountVectorizer()), ('classifier', MultinomialNB())])\n",
    "\n",
    "grid = [{\n",
    "    'vectorizer': [embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')], # parameters for combination of Count Vectorizer and Naive Bayes\n",
    "    'classifier': [MultinomialNB()]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')], # parameters for combination of Count Vectorizer and Logistic Regression\n",
    "    'classifier': [LogisticRegression(solver='liblinear')],\n",
    "    'classifier__C': [0.01, 1, 100] \n",
    "},\n",
    "{\n",
    "    'vectorizer': [embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')], # parameters for combination of Count Vectorizer and Support Vector Machine\n",
    "    'classifier': [SVC()],\n",
    "    'classifier__kernel': ['linear','rbf', 'poly'],\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [embeddingvectorizer.EmbeddingCountVectorizer(model, operator='mean')], # parameters for combination of Count Vectorizer and Random Forest\n",
    "    'classifier': [RandomForestRegressor()],\n",
    "    'classifier__max_features': [1-20],\n",
    "    'classifier__n_estimators': [10, 100, 1000]\n",
    "},    \n",
    "{\n",
    "    'vectorizer': [embeddingvectorizer.EmbeddingtdidfVectorizer(model, operator='mean')], # parameters for combination of Count Vectorizer and Naive Bayes\n",
    "    'classifier': [MultinomialNB()]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [embeddingvectorizer.EmbeddingTfidfVectorizer(model, operator='mean')], # parameters for combination of Count Vectorizer and Logistic Regression\n",
    "    'classifier': [LogisticRegression(solver='liblinear')],\n",
    "    'classifier__C': [0.01, 1, 100] \n",
    "},\n",
    "{\n",
    "    'vectorizer': [embeddingvectorizer.EmbeddingTfidfVectorizer(model, operator='mean')], # parameters for combination of Count Vectorizer and Support Vector Machine\n",
    "    'classifier': [SVC()],\n",
    "    'classifier__kernel': ['linear','rbf', 'poly'],\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "},\n",
    "{\n",
    "    'vectorizer': [embeddingvectorizer.EmbeddingTfidfVectorizer(model, operator='mean')], # parameters for combination of Count Vectorizer and Random Forest\n",
    "    'classifier': [RandomForestRegressor()],\n",
    "    'classifier__max_features': [1-20],\n",
    "    'classifier__n_estimators': [10, 100, 1000]\n",
    "}]\n",
    "                 \n",
    "search = GridSearchCV(estimator = pipe,\n",
    "                      param_grid = grid,\n",
    "                      scoring = 'accuracy',\n",
    "                      cv = 5,\n",
    "                      n_jobs = -1,\n",
    "                      verbose = 10)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "wv_SML_params = search.best_params_\n",
    "wv_SML_model = search.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best models with and without word embeddings can now be compared. The best model can then be chosen for prediction purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'These hyperparameters {reg_SML_params} provide the best performance using regular supervised machine learning techniques:')\n",
    "print(f\"\\n\\n\\n {classification_report(y_test, reg_SML_model)}\")\n",
    "print(f'\\n\\n These hyperparameters {wv_SML_params} provide the best performance using regular supervised machine learning techniques:')\n",
    "print(f\"\\n\\n {classification_report(y_test, wv_SML_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model to predict whether a paragraph is populist\n",
    "\n",
    "The data from the original research project contained manifestos that were not coded. One of these will be used to show how the model from above can be used to predict whether a paragraph is populist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use a dataframe containing only the paragraphs of the SP manifesto from 1998.\n",
    "SP1998 = pd.read_csv(\"/home/hennes/Downloads/SP1998.csv\")\n",
    "features = dataset['text'].tolist()\n",
    "\n",
    "# Next I clean its features and lemmatise them.\n",
    "clean(features)\n",
    "nl_lemmatise(features)\n",
    "\n",
    "# Finally I predict their features.\n",
    "populist = search.predict(docs).tolist()\n",
    "\n",
    "# And I add them to the dataframe and export the data.\n",
    "SP1998['Populist'] = pd.DataFrame({'Populist':populist})\n",
    "SP1998.to_csv(\"/home/hennes/Downloads/SP1998 with labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It could now be of interest to see how many of the paragraphs were labelled as populist.\n",
    "SP1998['Populist'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also be applied to a corpus of manifestos from different parties/years/countries etc. The results from the prediction could then be used for visualisation purposes. A example of this is a comparison of the percentage of populist paragraphs in a given manifesto. I am here showing how such a visualisation could look with the data that was already coded in the original research project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "dataset = pd.read_csv(directory+\"Master/NL/Combined Dataset.csv\")\n",
    "\n",
    "# Aggregating the data to show the percentage of populist paragraphs per manifesto\n",
    "dataset['Proportion Populist Paragraphs'] = dataset.Populist == 'Populist'\n",
    "perparty = dataset.groupby('Party/Year', as_index=False).agg({'Proportion Populist Paragraphs': 'mean'}).sort_values(['Proportion Populist Paragraphs'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This plot shows the Proportions per party, but in a dataset with multiple countries,\n",
    "# the percentage of populist paragraphs could also be plotted over different countries, years etc.\n",
    "fig = sns.barplot(x=\"Party/Year\", y=\"Proportion Populist Paragraphs\", data=perparty, ci=None)\n",
    "\n",
    "# Some settings to make the plot prettier, larger and more easily readable\n",
    "sns.set(style=\"whitegrid\", font_scale=1.3)\n",
    "plt.figure(figsize=(16,8))\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
